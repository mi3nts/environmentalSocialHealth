{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Environmental data processed. \n",
      "\n",
      "\n",
      "Creating Models... \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.gofplots import qqplot_2samples\n",
    "import os \n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import re\n",
    "\n",
    "root = \"../CAMS/\"\n",
    "units = \"../assets/CAMS_units.csv\"\n",
    "zip_2010 = \"../assets/tx_texas_zip_codes_geo.min.json\"\n",
    "hospital_data = \"/media/teamlary/ssd/Discharge Data/Inpatient/Data/\"\n",
    "census_dir = \"../Census/\"\n",
    "icd_data = \"../icd10/\"\n",
    "\n",
    "tx_zip = gpd.read_file(zip_2010)\n",
    "\n",
    "start_year = 2005\n",
    "end_year = 2022\n",
    "\n",
    "hospital_quarters = [f\"{year}q{quarter}\" for year in range(start_year, end_year + 1) for quarter in range(1, 5) if not (year == end_year and quarter > 2)]\n",
    "hospital_quarters = hospital_quarters[:-1]\n",
    "icd9_subset = hospital_quarters[hospital_quarters.index(f'{start_year}q1'):hospital_quarters.index('2015q4')]\n",
    "icd10_subset = hospital_quarters[hospital_quarters.index('2015q4'):hospital_quarters.index(f'{end_year}q1')]\n",
    "\n",
    "def numerical_sort(filename):\n",
    "    return [int(x) if x.isdigit() else x for x in re.split(r'(\\d+)', filename)]\n",
    "\n",
    "def grabWeatherData(time_period):\n",
    "    means = pd.DataFrame()\n",
    "    for quarter in time_period:\n",
    "        '''\n",
    "        Objective: create a dataframe of mean meterological values per zip code per quarter, over multiple quarters. \n",
    "        This way, we can merge multiple quarter data points with hospitalization data.\n",
    "        In one for loop, we may have to load mean zip code data for quarter, hospitalization data for quarter, merge, then delete the loaded files, and repeat\n",
    "\n",
    "        '''\n",
    "\n",
    "        ### load mean weather data into means (pd.DataFrame)\n",
    "        csvs = [root+quarter+'/'+i for i in os.listdir(root+quarter)]\n",
    "        sorted_csvs = sorted(csvs, key=numerical_sort)\n",
    "        # num_csvs = len(csvs)\n",
    "        for i in sorted_csvs:\n",
    "            df = pd.read_csv(i, skiprows=0, usecols=lambda x: x != 'Unnamed: 0')\n",
    "            means = pd.concat([means, df])\n",
    "        \n",
    "        # print(means.head())\n",
    "\n",
    "        # means.insert(0, 'PAT_ZIP', tx_zip['ZCTA5CE10'].values)\n",
    "    # means.columns = units_df['var_name'].values\n",
    "    means.insert(0, 'PAT_ZIP', tx_zip['ZCTA5CE10'].to_list()*len(time_period))\n",
    "    means.insert(1, 'LandArea_sqm', tx_zip['ALAND10'].to_list()*len(time_period))\n",
    "    means['PAT_ZIP'] = means['PAT_ZIP'].astype('int')\n",
    "\n",
    "    return means\n",
    "\n",
    "env_quarters = (pd.date_range(pd.to_datetime(f'{start_year}-01-01'), \n",
    "                   pd.to_datetime('2021-12-31') + pd.offsets.QuarterBegin(1), freq='Q')\n",
    "      .strftime('%Y-%m-%d')\n",
    "      .tolist()) # from 2005-03-31 to 2021-12-31\n",
    "\n",
    "env_data = grabWeatherData(env_quarters)\n",
    "second_index = [i for i in icd9_subset + icd10_subset for _ in range(1939)]\n",
    "env_data.insert(0, 'quarter', second_index)\n",
    "\n",
    "units_df = pd.read_csv(units)\n",
    "unit_names = ['quarter', 'PAT_ZIP','LandArea_sqm'] + list(units_df['var_name'].values[3:])\n",
    "env_data.columns = unit_names\n",
    "env_data = env_data.reset_index(drop=True)\n",
    "\n",
    "nice_names = dict(zip(env_data.columns[3:], units_df['long_name'][3:].values))\n",
    "nice_names['go3'] = 'Ozone mass mixing ratio'\n",
    "nice_names['ch4_c'] = 'Methane'\n",
    "nice_names['pop_density'] = 'Population Density'\n",
    "nice_names['pm10'] = 'Particulate matter 10um'\n",
    "nice_names['pm2p5'] = 'Particulate matter 2.5um'\n",
    "\n",
    "env_data = env_data.copy().dropna(axis=1, how='all')\n",
    "env_data = env_data.dropna()\n",
    "\n",
    "print(\"\\n\\n Environmental data processed. \\n\\n\")\n",
    "\n",
    "quarter_label = dict(zip(env_quarters, [i.upper() for i in icd9_subset + icd10_subset]))\n",
    "\n",
    "\n",
    "def eval_results(y_test, predictions, y_train, train_preds):\n",
    "    train_acc, test_acc = [], []\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    # print(\"RMSE: \", np.sqrt(mse))\n",
    "    train_r2 = r2_score(y_train, train_preds)\n",
    "    test_r2 = r2_score(y_test, predictions)\n",
    "    train_acc.append(train_r2)\n",
    "    test_acc.append(test_r2)\n",
    "    # print(\"Train R2: \", train_r2)\n",
    "    # print(\"Test R2: \", test_r2)\n",
    "    #print(train_preds)\n",
    "\n",
    "    # bin_lbl = bin_labels['labels'].loc[bin_labels['bin_id'] == item].values[0]\n",
    "\n",
    "    train_pdf = pd.DataFrame.from_dict({'Predicted': train_preds, 'Actual': y_train, 'Legend': 'Train'})\n",
    "    test_pdf = pd.DataFrame.from_dict({'Predicted': predictions, 'Actual': y_test, 'Legend': 'Test'})\n",
    "    full_pdf = pd.concat([train_pdf, test_pdf])\n",
    "    #print('len of full pdf inside fxn', len(full_pdf))\n",
    "    \n",
    "    return mse, train_r2, test_r2, train_acc, test_acc, full_pdf\n",
    "\n",
    "def XGB_model(X_train, X_test, y_train, y_test):\n",
    "    # dt = xgb.DMatrix(X_train, label=y_train.values)\n",
    "    # dv = xgb.DMatrix(X_test, label=y_test.values)\n",
    "    dt = xgb.DMatrix(X_train, label=y_train)\n",
    "    dv = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    params = {\n",
    "        # \"eta\": 0.05,\n",
    "        # \"max_depth\": 10,\n",
    "        # \"n_estimators\": 100,\n",
    "        # \"num_boost_round\": 10,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"device\": \"cuda\",\n",
    "        # \"verbose\": 0,\n",
    "        # \"verbosity\": 0,\n",
    "        # \"silent\": 1,\n",
    "        # \"base_score\": np.mean(y_test),\n",
    "        \"eval_metric\": \"rmse\"\n",
    "    }\n",
    "    # model = xgb.train(params, dt, 300, [(dt, \"train\"),(dv, \"valid\")])#, early_stopping_rounds=10)\n",
    "    model = xgb.train(\n",
    "    params=params, \n",
    "    dtrain=dt, \n",
    "    num_boost_round=300, \n",
    "    evals=[(dt, \"train\"), (dv, \"valid\")],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=False  # You can set verbose_eval to True or False as needed\n",
    ")\n",
    "\n",
    "    \n",
    "    predictions = model.predict(dv)\n",
    "    train_preds = model.predict(dt)\n",
    "    \n",
    "    return model, predictions, train_preds\n",
    "\n",
    "def test_train_plot(full_pdf, y_test, train_r2, y_train, X_train, test_r2, X_test, title, save_path):\n",
    "    plt.figure(figsize=(12, 8)) \n",
    "    print('inside r2 plot')\n",
    "    #print('len of full pdf inside plot fxn', len(full_pdf)) \n",
    "    \n",
    "    g = sns.jointplot(data=full_pdf, x='Predicted', y='Actual', hue='Legend', alpha=0.3, #cmap=\"Blues\", \n",
    "                          marginal_kws=dict(bw_adjust=0.2, cut=0))\n",
    "    g.set_axis_labels('Predicted ($\\log_{10}$(# ICD Codes/Zip Code Population))', 'Actual ($\\log_{10}$(# ICD Codes/Zip Code Population))')\n",
    "    set_min_plot = y_train\n",
    "    g.ax_joint.plot([min(set_min_plot), max(set_min_plot)], [min(set_min_plot), max(set_min_plot)], color='k',label=\"1:1\")\n",
    "    handles, labels = g.ax_joint.get_legend_handles_labels()\n",
    "    g.ax_joint.legend(handles=handles, labels=[ f'Train: {train_r2:.2f}, N = {len(X_train):,}', f'Test: {test_r2:.2f}, N={len(X_test):,}','1:1',], title=None)\n",
    "    g.fig.suptitle(title, size=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0, wspace=0)\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    # plt.show()\n",
    "\n",
    "    \n",
    "def plot_qq(full_pdf, save_path):\n",
    "    print('inside qq plot')\n",
    "    fig = qqplot_2samples(full_pdf['Predicted'],full_pdf['Actual'],line='45', \n",
    "                    xlabel='Predicted ($\\log_{10}$(# ICD Codes/Zip Code Population))',  \n",
    "                    ylabel='Actual ($\\log_{10}$(# ICD Codes/Zip Code Population))')\n",
    "    plt.title(f'Quantile-Quantile plot')\n",
    "\n",
    "    # plt.show()\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "\n",
    "def shap_plots(model, X_test, save_path, title):\n",
    "    print('inside shap plot')\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "\n",
    "    # feature_names = [nice_names[i] for i in X_test.columns]\n",
    "    # print(feature_names)\n",
    "    # feature_names = X_test.columns\n",
    "    feature_names = {i: nice_names[i] for i in X_test.columns}\n",
    "    # print([(i,j,nice_names[j]) for i,j in  enumerate(feature_names)])\n",
    "    # print(feature_names)\n",
    "    # X_test = pd.DataFrame(X_test)\n",
    "    X_test.columns = [i if i not in feature_names.keys() else feature_names[i] for i in feature_names]\n",
    "    # print(X_test.head)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    # plt.figure(figsize=(18,12))\n",
    "    shap.summary_plot(shap_values, X_test, plot_type='dot', max_display=10, show=False, plot_size=[16,8])\n",
    "    # print('part2')\n",
    "    # fig, ax = plt.gcf(), plt.gca()\n",
    "    feature_order = np.argsort(np.sum(np.abs(shap_values), axis=0))\n",
    "    # print(feature_order, X_test.columns[feature_order])\n",
    "    # ax.set_yticklabels([corr_labels[i] for i in selected_columns])\n",
    "    # ax.set_yticklabels([selected_columns[i] for i in feature_order])\n",
    "\n",
    "    # ax.set_yticklabels([corr_labels[i] for i in features.columns[feature_order]])\n",
    "    plt.xlabel('Average Impact on Model')\n",
    "    # plt.title(f'Feature Importance Ranking for {plot_title}{pm_labels[item]}')\n",
    "    # print(f\"Shap summary for {item}\")\n",
    "    plt.title(f'{title}') \n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    # plt.show()\n",
    "\n",
    "icd_codes = ['I5042', 'A4189', 'I509', 'I2510','F209','G309', 'J4521']\n",
    "icd_codes = os.listdir('../icd10/')\n",
    "\n",
    "def getDF(icd_codes):\n",
    "    fits_data = []\n",
    "    nthresh = 3\n",
    "    data_list = []\n",
    "    for icd_code in icd_codes:\n",
    "        df = pd.DataFrame()\n",
    "        for quarter in hospital_quarters[:-1]: # [:-1] to avoid 2022q1 \n",
    "\n",
    "            # read in icd data\n",
    "            icd_df = pd.read_csv(f'{icd_data+icd_code}/{quarter}.csv') \n",
    "            \n",
    "            # environmental data is already read in\n",
    "            # for each quarter, .loc env_data that is on that quarter, then \n",
    "            # merge with zip codes that are in icd_df\n",
    "            # this merged df needs to be concat into df for every quarter\n",
    "            env_df = env_data[env_data['quarter'] == quarter]\n",
    "            \n",
    "            # census_df = census_data[census_data['year'] == quarter[:4]]\n",
    "            # env_icd = env_df.merge(icd_df, on='PAT_ZIP')\n",
    "\n",
    "            # full_df = census_df.merge(env_icd, on='PAT_ZIP')\n",
    "            full_df = env_df.merge(icd_df, on='PAT_ZIP')\n",
    "            full_df['pop_density'] = full_df['population']/(full_df['LandArea_sqm']/1_000_000)\n",
    "        \n",
    "\n",
    "            df = pd.concat([df, full_df])\n",
    "        \n",
    "        os.makedirs(f'./Plots/{icd_code}', exist_ok=True)\n",
    "\n",
    "        data = df.copy()\n",
    "        data_quality = data.reset_index(drop=True)\n",
    "        # data_quality = data_quality.drop(census_data.columns[2:], axis=1) # dropping all census data\n",
    "        # print(len(data_quality))\n",
    "        data_quality = data_quality[data_quality['ICD'] >= nthresh]\n",
    "        #print(f'Len of data at no threshold: {len(data)}, len of data at threshold: {len(data_quality)}')\n",
    "        \n",
    "        # data_quality = data_quality.drop([data_quality.columns[data_quality.isna().any()]][0], axis=1)\n",
    "        # print(len(data_quality))\n",
    "        # data_quality = np.log10(data_quality)\n",
    "        \n",
    "        data_quality.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        data_quality = data_quality.dropna(axis=0)\n",
    "        data_list.append(data_quality)\n",
    "        # print(data_quality.columns)\n",
    "        # print(len(data_quality))\n",
    "        # data_quality = data_quality[data_quality['median household income'] > 0]\n",
    "        # X = data_quality.drop(['PAT_ZIP','ICD','normalized', 'population', 'quarter'], axis=1)\n",
    "        # X = data_quality.drop(['LandArea_sqm'], axis=1) # dropping land area in sqm\n",
    "        # print(len(X))\n",
    "\n",
    "        # \"chnk\", \\\n",
    "        # X = data_quality.loc[:,[\"d2m\",\"t2m\", \"lai_hv\",\"lai_lv\", \n",
    "        # \"pm10\",\"pm2p5\",\"stl1\",\n",
    "        # \"sp\",\n",
    "        # \"co\", \"aermr04\",\"aermr05\",\"aermr06\", \n",
    "        # \"c2h6\",\"hcho\",\"aermr09\",\"aermr07\",\"aermr10\",\n",
    "        # \"aermr08\",\"oh\", \"c5h8\", \"ch4_c\",\n",
    "        # \"hno3\",\"no2\",\"no\",\"go3\",\"pan\",\n",
    "        # \"c3h8\", \"aermr01\",\"aermr02\",\"aermr03\",\n",
    "        # \"aermr12\",\n",
    "        # \"aermr11\",\n",
    "        # \"so2\"]] #, \"pop_density\"]] # including only relevant environmental data\n",
    "        # # \"median household income\", \\\n",
    "        # # \"hispanic\",\n",
    "        # # # \"aggregate income\",\\\n",
    "        # # \"males college < 1yr\", \\\n",
    "        # # \"males college > 1yr, no degree\",\"males associate degree\", \"SNAP eligibility\",]]\n",
    "        # # X = X.dropna()\n",
    "        # # print(X.shape)\n",
    "\n",
    "        # # data_quality['normalized'].plot()\n",
    "\n",
    "        # # return df\n",
    "        # sns.set_style('ticks')\n",
    "        # # rmse_list = []\n",
    "        # # for i in range(500):\n",
    "        # y = np.log10(data_quality['normalized'])\n",
    "        # # y = data_quality['normalized']\n",
    "        # # y = data_quality['ICD']\n",
    "        # # seed = 140\n",
    "\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #, random_state=seed)\n",
    "\n",
    "        # feature_scaler = MinMaxScaler()\n",
    "        # X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "        # X_test_scaled = feature_scaler.transform(X_test)\n",
    "\n",
    "        # target_scaler = MinMaxScaler()\n",
    "        # y_train_scaled = target_scaler.fit_transform(y_train.to_numpy().reshape(-1,1))\n",
    "        # y_test_scaled = target_scaler.transform(y_test.to_numpy().reshape(-1,1))\n",
    "\n",
    "        \n",
    "        # # model = GradientBoostingRegressor()\n",
    "\n",
    "        # # model = ExtraTreesRegressor()\n",
    "\n",
    "        # # Train and predict using each model\n",
    "        \n",
    "        # # for model_name, model in models.items():\n",
    "        # # print('X_train', len(X_train_scaled), ' y_test', len(y_test))\n",
    "        # # model.fit(X_train_scaled, y_train)\n",
    "        # # y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        # # r2_scores = r2_score(y_test, y_pred)\n",
    "        # # print(r2_scores)\n",
    "        # # rmse_list.append(r2_scores)\n",
    "\n",
    "        # # -----------\n",
    "\n",
    "        # # Get feature importances\n",
    "\n",
    "        # # things that worked\n",
    "\n",
    "        # # model = RandomForestRegressor()\n",
    "        # # predictions = {}\n",
    "        # # model.fit(X_train_scaled, y_train)\n",
    "        # icd_code_title = icd_code\n",
    "        # # y_pred = model.predict(X_test_scaled)\n",
    "        # # r2_scores = r2_score(y_test, y_pred)\n",
    "        # # train_preds = model.predict(X_train_scaled)\n",
    "\n",
    "        # model, y_pred, train_preds = XGB_model(X_train, X_test, \n",
    "        #                                        y_train, y_test)\n",
    "\n",
    "\n",
    "        # mse, train_r2, test_r2, train_acc, test_acc, full_pdf = eval_results(y_test, y_pred, y_train, train_preds)\n",
    "        # # print('full pdf', len(full_pdf))\n",
    "        # print('sus error start')\n",
    "    \n",
    "        # test_train_plot(full_pdf, y_test, train_r2, y_train, X_train_scaled, test_r2, X_test_scaled,\n",
    "        #                 title=f\"ICD-10 Codes for {icd_code_labels[icd_code_title]} \\n # threshold = {nthresh}, Environmental data \\n from {start_year} to {end_year-1}\",\n",
    "        #                 save_path=f'./Plots/{icd_code}/{icd_code}_r2.png'\n",
    "        #                 )\n",
    "        # print('sus error end')\n",
    "        \n",
    "        \n",
    "        # # don't use with XGBoost\n",
    "\n",
    "        # # importance_scores = model.feature_importances_\n",
    "        # # feature_names = [nice_names[i] for i in X.columns]\n",
    "        # # # feature_names = X.columns\n",
    "        # # # feature_names = [(i, nice_names[i]) for i in X.columns]\n",
    "\n",
    "        # # # Sort feature importances in descending order\n",
    "        # # indices = importance_scores.argsort()[::-1][:20]\n",
    "        # # sorted_feature_names = ([feature_names[i] for i in indices])\n",
    "        # # sorted_importance_scores = (importance_scores[indices])\n",
    "\n",
    "        # # # Create a horizontal bar chart of feature importances\n",
    "        # # plt.figure(figsize=(10, 6))\n",
    "        # # plt.barh(range(len(sorted_importance_scores)), sorted_importance_scores[::-1], align='center')\n",
    "        # # plt.yticks(range(len(sorted_importance_scores))[::-1], sorted_feature_names)\n",
    "\n",
    "\n",
    "\n",
    "        # # plt.title(f'Feature Importance Ranking for Environmental data model on {icd_code_labels[icd_code_title]}')\n",
    "        # # plt.ylabel('Features')\n",
    "        # # plt.xlabel('Feature Importance Ranking')\n",
    "        # # # plt.savefig(f'./Plots/{icd_code}/{icd_code}_feat_imp.png')\n",
    "        # # plt.show()\n",
    "        # # print(rmse_list)\n",
    "\n",
    "        # shap_plots(model, \n",
    "        #     X_test, \n",
    "        #     f'./Plots/{icd_code}/{icd_code}_shap.png', \n",
    "        #     title=f\"SHAP Values for {icd_code_labels[icd_code_title]}\")\n",
    "\n",
    "        # plot_qq(full_pdf, f'./Plots/{icd_code}/{icd_code}_qq.png')\n",
    "\n",
    "        # del full_pdf, df\n",
    "\n",
    "        # fits_data.append([icd_code, train_r2, test_r2, np.sqrt(mse), len(X)])\n",
    "\n",
    "    # fits_df = pd.DataFrame(fits_data)\n",
    "    # fits_df.columns = ['ICD', 'train_r2', 'test_r2', 'rmse', 'numDataPoints']\n",
    "\n",
    "    return data_list\n",
    "\n",
    "\n",
    "    # fits_df.to_csv(f'n_thresh = {nthresh} - results.csv')\n",
    "\n",
    "icd_code_labels = {\n",
    "    'A4189': 'A41.89 - Other Specified Sepsis',\n",
    "    'B9789': 'B97.89 - Other viral agents as the cause \\n of diseases classified elsewhere',\n",
    "    'J157' : 'J15.7 - Pneumonia due to Mycoplasma pneumoniae',\n",
    "    'J449' : 'J44.9 - Chronic obstructive pulmonary disease, unspecified',\n",
    "    'J45998' : 'J45.998 - Other asthma',\n",
    "    'J4521' : 'J45.21 - Mild intermittent asthma \\n with (acute) exacerbation', \n",
    "    'F068' : 'F06.8 - Other specified mental disorders \\n due to known physiological condition', \n",
    "    'I2510' : 'I25.10 - Atherosclerotic heart disease \\n of native coronary artery without angina pectoris',\n",
    "    'I509' : 'I50.9 - Heart failure, unspecified', \n",
    "    'A4189' : 'A41.89 - Other specified sepsis',\n",
    "    'I5041' : 'I50.41 - Acute combined systolic (congestive) \\n and diastolic (congestive) heart failure',\n",
    "    'I5042' : 'I50.42 - Chronic combined systolic (congestive) \\n diastolic (congestive) heart failure',\n",
    "    'F209' : 'Schizophrenia, unspecified'\n",
    "}\n",
    "\n",
    "# icd_codes = ['A4189', 'I5042','I509', 'I2510','F209','G309', 'J4521']\n",
    "icd_codes = ['A419','I2510','E860','J189']\n",
    "icd_codes_subset = list(icd_code_labels.keys())[:1]\n",
    "\n",
    "print(\"Creating Models... \\n\\n\")\n",
    "data_for_icd = getDF(icd_codes[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_icd[0].to_csv('../assets/A419.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bceb9cf4561ca3ef8bb7a721829de9548ce9f86c87a3b466c7f7299bf7881a99"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('hospitalization')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
